{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J:\\\\My TIME\\\\Data Scientist\\\\MyPython-programming\\\\Machine Learning examples\\\\NLP\\\\nlp'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['VIRTUAL_ENV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'a', 'book', '.', 'It', 'is', 'Python', 'Machine', 'Learning', 'By', 'Example', ',', '2nd', 'edition', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.tokenize import sent_tokenize # токенизация значений\n",
    "sent = '''I am reading a book.\n",
    "    It is Python Machine Learning By Example,\n",
    "    2nd edition.'''\n",
    "print(word_tokenize(sent))\n",
    "# print(sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7430c70f2a1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msent2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'I live in Kyiv and it\\'s fun'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokens2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print([token.text for token in tokens2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sent2 = 'I live in Kyiv and it\\'s fun'\n",
    "tokens2 = nlp(sent2)\n",
    "# print([token.text for token in tokens2])\n",
    "tokens  = word_tokenize(sent2)\n",
    "print(nltk.pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Roman', 'GPE'), ('2018', 'DATE'), ('30', 'MONEY'), ('Roman', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition - опеределяет сщуности\n",
    "tokens3 = nlp('The book written in Roman in 2018 was sold at $30 in Roman')\n",
    "print([(token_ent.text, token_ent.label_) for token_ent in tokens3.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:\\My TIME\\Data Scientist\\MyPython-programming\\Machine Learning examples\\NLP\\nlp\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studi'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# нахождение крней слов в речи(алгоритм POS)\n",
    "# остальные два алгоритма LancasterStemmer and SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  =  PorterStemmer()\n",
    "porter_stemmer.stem('fully')\n",
    "porter_stemmer.stem('studying')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Why is learning unchanged? It turns out that this algorithm only lemmatizes on\n",
    "# nouns by default.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('machines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The first project in this book is about the 20 newsgroups dataset. It's composed of text taken\n",
    "from newsgroup articles, as its name implies. It was originally collected by Ken Lang and\n",
    "now has been widely used for experiments in text applications of machine learning\n",
    "techniques, specifically NLP techniques. \"\"\"\n",
    "# The data contains approximately 20,000 documents across 20 online newsgroups."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bc169737faf989233e48e1c3aba26ce9234ebf0dafa5a40eebf1f228f24c604"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('datamech': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
